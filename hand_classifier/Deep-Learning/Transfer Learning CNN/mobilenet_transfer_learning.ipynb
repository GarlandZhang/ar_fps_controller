{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning example using Keras and Mobilenet# \n",
    "In this notebook I shall show you an example of using Mobilenet to classify images of dogs. I will then show you an example when it subtly misclassifies a bluetit. I will then retrain Mobilenet and employ transfer learning such that it can correctly classify the same input image.  Only two classifiers are employed. But this can be extended to as many as you want, limited to the amount of hardware and time you have available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "# from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall be using Mobilenet as it is lightweight\n",
    "<img src=\"files/MobileNet architecture.png\">\n",
    "\n",
    "\n",
    "It is also very low maintence.\n",
    "<img src=\"files/mobilenet_v1.png\">\n",
    "\n",
    "Source paper located here: https://arxiv.org/pdf/1704.04861.pdf\n",
    "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\n",
    "Applications, Howard et al, 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobile = tf.keras.applications.mobilenet.MobileNet()\n",
    "# def prepare_image(file):\n",
    "#     img_path = ''\n",
    "#     img = image.load_img(img_path + file, target_size=(224, 224))\n",
    "#     img_array = image.img_to_array(img)\n",
    "#     img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "#     return keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)\n",
    "\n",
    "# base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "# x=base_model.output\n",
    "# x=GlobalAveragePooling2D()(x)\n",
    "# x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "# x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "# x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "# preds=Dense(14,activation='softmax')(x) #final layer with softmax activation\n",
    "# model=Model(inputs=base_model.input,outputs=preds)\n",
    "# #specify the inputs\n",
    "# #specify the outputs\n",
    "# #now a model has been created based on our architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pre-trained weights as the model has been trained already on the Imagenet dataset. We ensure all the weights are non-trainable. We will only train the last few dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('mobilenet_set(4)_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "# or if we want to set the first 20 layers of the network to be non-trainable\n",
    "# for layer in model.layers[:20]:\n",
    "#     layer.trainable=False\n",
    "for layer in model.layers[:-5]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load the training data into the ImageDataGenerator. Specify path, and it automatically sends the data for training in batches, simplifying the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    'curl',  \n",
    "    'curl_45_out', \n",
    "    'curl_45_palm', \n",
    "    'curl_90_out', \n",
    "    'curl_90_palm', \n",
    "    'point_foreward', \n",
    "    'point_foreward_45_out', \n",
    "    'point_foreward_45_palm', \n",
    "    'point_foreward_90_out', \n",
    "    'point_foreward_90_palm',\n",
    "    'point_up',\n",
    "    'point_up_out',\n",
    "    'point_up_palm',\n",
    "    'point_up_thumb'\n",
    "]\n",
    "train_datagen=ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.5\n",
    ") #included in our dependencies\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory('E:/ar_fps/data_collection/datasets_single_perspective/train',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 batch_size=8,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 classes=class_labels,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "valid_generator = valid_datagen.flow_from_directory('E:/ar_fps/data_collection/datasets_single_perspective/validation', \n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=8,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    classes=class_labels,\n",
    "                                                    shuffle=True\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory='E:/ar_fps/data_collection/datasets_single_perspective/test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=8,\n",
    "#     class_mode=None,\n",
    "    classes=class_labels,\n",
    "    shuffle=False,\n",
    "#     seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model. Now lets train it. Should take less than two minutes on a GTX1070 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    history = model.fit_generator(generator=train_generator,\n",
    "                        validation_data=valid_generator,\n",
    "                       steps_per_epoch=step_size_train,\n",
    "                       epochs=1)\n",
    "    model.save('mobilenet_set(4)_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(generator=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(30)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is now trained. Now lets test some independent input images to check the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(150, 150))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ind = 11\n",
    "img_dir = 'E:/ar_fps/data_collection/datasets_single_perspective/test/{}'.format(class_labels[label_ind])\n",
    "print(class_labels[label_ind])\n",
    "img_files = os.listdir(img_dir)\n",
    "correct = 0\n",
    "for img_name in img_files:\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    new_image = load_image(img_path)\n",
    "    pred = model.predict(new_image)\n",
    "    if np.argmax(pred[0]) == label_ind:\n",
    "        correct += 1\n",
    "    print(\"{} | {}\".format(class_labels[np.argmax(pred[0])], pred))\n",
    "correct / len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#img_path = 'C:/Users/Ferhat/Python Code/Workshop/Tensoorflow transfer learning/blue_tit.jpg'\n",
    "label_ind = 1\n",
    "img_dir = 'E:/ar_fps/data_collection/datasets_single_perspective/train/{}'.format(class_labels[label_ind])\n",
    "print(class_labels[label_ind])\n",
    "img_files = os.listdir(img_dir)\n",
    "correct = 0\n",
    "for img_name in img_files:\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    new_image = load_image(img_path)\n",
    "    pred = model.predict(new_image)\n",
    "    if np.argmax(pred[0]) == label_ind:\n",
    "        correct += 1\n",
    "    print(\"{} | {}\".format(class_labels[np.argmax(pred[0])], pred))\n",
    "correct / len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mobilenet_many_angles_new4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = lite.TFLiteConverter.from_keras_model_file('mobilenet_many_angles_new3.h5', \n",
    "                                                       input_shapes={\n",
    "                                                           'input_2': [1, 224, 224, 3],\n",
    "                                                       }\n",
    "            )\n",
    "tfmodel = converter.convert()\n",
    "open(\"model.tflite\",\"wb\").write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
