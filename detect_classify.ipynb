{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Softmax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(y_true, y_pred):\n",
    "     return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('hand_classifier/model_simple', custom_objects={'foo': foo})\n",
    "gt_path = 'hand_classifier/frames/curl_annotations'\n",
    "frames_path = 'hand_classifier/frames/curl'\n",
    "video_path = 'test_video.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('cropped/img98.jpg')\n",
    "# img = cv2.resize(img, (75, 75))\n",
    "# batch = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "# res = model.predict(batch)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(xml_file):\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    data = ET.parse(xml_file)\n",
    "    root = data.getroot()\n",
    "    objs = root.findall('object')\n",
    "    for obj in objs:\n",
    "        box = obj.find('bndbox')\n",
    "        xmin = int(box.find('xmin').text)\n",
    "        ymin = int(box.find('ymin').text)\n",
    "        xmax = int(box.find('xmax').text)\n",
    "        ymax = int(box.find('ymax').text)\n",
    "        box = [xmin, ymin, xmax, ymax]\n",
    "        boxes.append(box)\n",
    "        \n",
    "        name = root.find('class_name') # TEMPORARY; made a bug by mistake\n",
    "#         name = obj.find('class_name')\n",
    "        if name is None:\n",
    "            name = obj.find('name')\n",
    "        classes.append(name.text)\n",
    "    return boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect graph\n",
    "# ckpt_path = \"handtracking/handtracking/hand_inference_graph/frozen_inference_graph.pb \"\n",
    "# detection_graph = tf.Graph()\n",
    "# with detection_graph.as_default():\n",
    "#     od_graph_def = tf.compat.v1.GraphDef()\n",
    "#     with tf.io.gfile.GFile(ckpt_path, 'rb') as fid:\n",
    "#         serialized_graph = fid.read()\n",
    "#         od_graph_def.ParseFromString(serialized_graph)\n",
    "#         tf.import_graph_def(od_graph_def, name='')\n",
    "#     sess = tf.compat.v1.Session(graph=detection_graph)\n",
    "\n",
    "\n",
    "frames = []\n",
    "# ground truth boxes \n",
    "\n",
    "# video\n",
    "# cap = cv2.VideoCapture(video_path)  \n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     frames.append(frame)\n",
    "# cap.release()\n",
    "\n",
    "# from directory\n",
    "frame_files = os.listdir(frames_path)\n",
    "for frame_file in frame_files:\n",
    "    frame = cv2.imread(os.path.join(frames_path, frame_file))\n",
    "    frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'hand_classifier/frames'\n",
    "# frames = os.listdir('test')\n",
    "labels = ['point_foreward', 'point_up']\n",
    "gt_files = os.listdir(gt_path)\n",
    "failed_dir = 'detect_classify_images/failed/'\n",
    "train_dir = 'hand_classifier/images/train'\n",
    "train_class_dir = os.path.join(train_dir, os.path.basename(frames_path))\n",
    "# num_train = len(os.listdir(train_class_dir))\n",
    "# offset = num_train\n",
    "num_train = 0\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frames))\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind, image in enumerate(frames):\n",
    "#     image = cv2.imread(os.path.join(src_dir, image_name))\n",
    "    image_inp = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    im_height, im_width, _ = image_inp.shape\n",
    "\n",
    "    # plt.imshow(image)\n",
    "\n",
    "    # detect setup\n",
    "#     image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "#     detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "#     detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "#     detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "#     num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "#     batch = np.expand_dims(image_inp, axis=0) \n",
    "\n",
    "#     boxes, scores, classes, num = sess.run(\n",
    "#       [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "#       feed_dict={image_tensor: batch}\n",
    "#     )\n",
    "\n",
    "#     boxes = np.squeeze(boxes)\n",
    "#     scores = np.squeeze(scores)\n",
    "\n",
    "#     i = np.argmax(scores)\n",
    "#     left = int(boxes[i][1] * im_width)\n",
    "#     right = int(boxes[i][3] * im_width)\n",
    "#     top = int(boxes[i][0] * im_height)\n",
    "#     bottom = int(boxes[i][2] * im_height)\n",
    "\n",
    "    # detect with ground truth\n",
    "    gt_file = os.path.join(gt_path, gt_files[ind])\n",
    "    boxes, classes = read_xml(gt_file)\n",
    "    left, top, right, bottom = boxes[0]\n",
    "    act_left, act_top, act_right, act_bottom = boxes[0]\n",
    "    actual_label = classes[0]\n",
    "\n",
    "    cropped = image[top:bottom, left:right]\n",
    "    \n",
    "#     cv2.imwrite('cropped/img{}.jpg'.format(str(ind)), cropped)\n",
    "    \n",
    "    # classifier\n",
    "    img = cv2.resize(cropped, (75, 75))\n",
    "    batch = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "    res = model.predict(batch)\n",
    "    res_ind = np.argmax(res[0])\n",
    "    print(res_ind)\n",
    "    name = labels[res_ind]\n",
    "    \n",
    "    p1 = (int(left), int(top))\n",
    "    p2 = (int(right), int(bottom))\n",
    "    disp = np.copy(image)\n",
    "    cv2.rectangle(disp, p1, p2, (77, 255, 9), 3, 1)\n",
    "#     cv2.putText(disp, \"score: {}\".format(scores[i]), p1, cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)\n",
    "    cv2.putText(disp, name, (left, top + 10), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)\n",
    "    cv2.imwrite('detect_classify_images/output/img{}.jpg'.format(str(ind + offset)), disp)\n",
    "    \n",
    "    # isolate failed images to train on again \n",
    "    if name != actual_label:\n",
    "        print('{}: prediction is {} | actual is {}'.format(ind, name, actual_label))\n",
    "        failed_class_dir = os.path.join(failed_dir, actual_label)\n",
    "        actual_box = image[act_top:act_bottom, act_left:act_right]\n",
    "        if not os.path.exists(failed_class_dir):\n",
    "            os.makedirs(failed_class_dir)\n",
    "            \n",
    "        cv2.imwrite(os.path.join(failed_class_dir, 'failed_img{}.jpg'.format(str(ind + offset))), actual_box)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# live demo\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    img = cv2.resize(frame, (75, 75))\n",
    "    batch = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "    res = model.predict(batch)\n",
    "    name = labels[np.argmax(res[0])]\n",
    "#     print(name)\n",
    "    \n",
    "    cv2.putText(frame, name, (40, 40), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
