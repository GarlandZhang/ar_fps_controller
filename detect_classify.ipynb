{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import copy\n",
    "from hand_classifier.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from object_detection.darkflow.darkflow.net.build import TFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Softmax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('hand_classifier/Deep-Learning/Transfer Learning CNN/mobilenet_many_angles_new4.h5')\n",
    "gt_path = 'hand_classifier/frames/curl_annotations'\n",
    "frames_path = 'hand_classifier/frames/curl'\n",
    "video_path = 'test_video.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'gpu': 1.0,\n",
    "    'model': 'object_detection/darkflow/cfg/yolov2-tiny-voc-hand.cfg',\n",
    "    'load': 56440,\n",
    "    'threshold': 0.1,\n",
    "    'backup': 'E:/ar_fps/object_detection/ckpt',\n",
    "}\n",
    "detector = TFNet(options)\n",
    "detector.load_from_ckpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('cropped/img98.jpg')\n",
    "# img = cv2.resize(img, (75, 75))\n",
    "# batch = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "# res = model.predict(batch)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(xml_file):\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    data = ET.parse(xml_file)\n",
    "    root = data.getroot()\n",
    "    objs = root.findall('object')\n",
    "    for obj in objs:\n",
    "        box = obj.find('bndbox')\n",
    "        xmin = int(box.find('xmin').text)\n",
    "        ymin = int(box.find('ymin').text)\n",
    "        xmax = int(box.find('xmax').text)\n",
    "        ymax = int(box.find('ymax').text)\n",
    "        box = [xmin, ymin, xmax, ymax]\n",
    "        boxes.append(box)\n",
    "        \n",
    "#         name = root.find('class_name') # TEMPORARY; made a bug by mistake\n",
    "        name = obj.find('class_name')\n",
    "        if name is None:\n",
    "            name = obj.find('name')\n",
    "        classes.append(name.text)\n",
    "    return boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect graph\n",
    "# ckpt_path = \"handtracking/handtracking/hand_inference_graph/frozen_inference_graph.pb \"\n",
    "# detection_graph = tf.Graph()\n",
    "# with detection_graph.as_default():\n",
    "#     od_graph_def = tf.compat.v1.GraphDef()\n",
    "#     with tf.io.gfile.GFile(ckpt_path, 'rb') as fid:\n",
    "#         serialized_graph = fid.read()\n",
    "#         od_graph_def.ParseFromString(serialized_graph)\n",
    "#         tf.import_graph_def(od_graph_def, name='')\n",
    "#     sess = tf.compat.v1.Session(graph=detection_graph)\n",
    "\n",
    "\n",
    "frames = []\n",
    "# ground truth boxes \n",
    "\n",
    "# video\n",
    "# cap = cv2.VideoCapture(video_path)  \n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "#     frames.append(frame)\n",
    "# cap.release()\n",
    "\n",
    "# from directory\n",
    "frame_files = os.listdir(frames_path)\n",
    "for frame_file in frame_files:\n",
    "    frame = cv2.imread(os.path.join(frames_path, frame_file))\n",
    "    frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'E:/ar_fps/action_perception/frames'\n",
    "# frames = os.listdir('test')\n",
    "labels = get_class_labels()\n",
    "gt_files = os.listdir(gt_path)\n",
    "failed_dir = 'detect_classify_images/failed/'\n",
    "train_dir = 'hand_classifier/images/train'\n",
    "train_class_dir = os.path.join(train_dir, os.path.basename(frames_path))\n",
    "# num_train = len(os.listdir(train_class_dir))\n",
    "# offset = num_train\n",
    "num_train = 0\n",
    "offset = 0\n",
    "assets_dir = 'C:/../Garland\\'s Pixel/Internal shared storage/ar_fps'\n",
    "asset_img_path = os.path.join(assets_dir, 'test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frames))\n",
    "print(offset)\n",
    "print(get_class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total_time = 0\n",
    "write_img = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind, image in enumerate(frames):\n",
    "    \n",
    "    # detect with ground truth\n",
    "    gt_file = os.path.join(gt_path, gt_files[ind])\n",
    "    boxes, classes = read_xml(gt_file)\n",
    "    left, top, right, bottom = boxes[0]\n",
    "    act_left, act_top, act_right, act_bottom = boxes[0]\n",
    "    actual_label = classes[0]\n",
    "\n",
    "    cropped = image[top:bottom, left:right]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # detect with handtracking object detector\n",
    "#     image = cv2.imread(os.path.join(src_dir, image_name))\n",
    "#     image_inp = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     im_height, im_width, _ = image_inp.shape\n",
    "\n",
    "    # plt.imshow(image)\n",
    "\n",
    "#     image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "#     detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "#     detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "#     detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "#     num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "#     batch = np.expand_dims(image_inp, axis=0) \n",
    "\n",
    "#     boxes, scores, classes, num = sess.run(\n",
    "#       [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "#       feed_dict={image_tensor: batch}\n",
    "#     )\n",
    "\n",
    "#     boxes = np.squeeze(boxes)\n",
    "#     scores = np.squeeze(scores)\n",
    "\n",
    "#     i = np.argmax(scores)\n",
    "#     left = int(boxes[i][1] * im_width)\n",
    "#     right = int(boxes[i][3] * im_width)\n",
    "#     top = int(boxes[i][0] * im_height)\n",
    "#     bottom = int(boxes[i][2] * im_height)\n",
    "\n",
    "\n",
    "    # detect with actual object detector\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.return_predict(img)\n",
    "\n",
    "    \n",
    "    # send to android app\n",
    "    start_write_img = time.time()\n",
    "    cv2.imwrite(asset_img_path, cropped)\n",
    "#     cv2.imwrite('cropped/img{}.jpg'.format(str(ind)), cropped)\n",
    "    end_write_img = time.time()\n",
    "    write_img += end_write_img - start_write_img\n",
    "    \n",
    "    # wait til get response\n",
    "    print('wait til get response')\n",
    "    \n",
    "    # classifier\n",
    "#     img = cv2.resize(cropped, (75, 75))\n",
    "#     batch = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "#     res = model.predict(batch)\n",
    "#     res_ind = np.argmax(res[0])\n",
    "# #     print(res_ind)\n",
    "#     name = labels[res_ind]\n",
    "    \n",
    "#     p1 = (int(left), int(top))\n",
    "#     p2 = (int(right), int(bottom))\n",
    "#     disp = np.copy(image)\n",
    "#     cv2.rectangle(disp, p1, p2, (77, 255, 9), 3, 1)\n",
    "# #     cv2.putText(disp, \"score: {}\".format(scores[i]), p1, cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)\n",
    "#     cv2.putText(disp, name, (left, top + 10), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,0), 1)\n",
    "#     cv2.imwrite('detect_classify_images/output/img{}.jpg'.format(str(ind + offset)), disp)\n",
    "    \n",
    "#     # isolate failed images to train on again \n",
    "#     if name != actual_label:\n",
    "#         print('{}: prediction is {} | actual is {}'.format(ind, name, actual_label))\n",
    "#         failed_class_dir = os.path.join(failed_dir, actual_label)\n",
    "#         actual_box = image[act_top:act_bottom, act_left:act_right]\n",
    "#         if not os.path.exists(failed_class_dir):\n",
    "#             os.makedirs(failed_class_dir)\n",
    "            \n",
    "#         cv2.imwrite(os.path.join(failed_class_dir, 'failed_img{}.jpg'.format(str(ind + offset))), actual_box)\n",
    "    end_time = time.time()\n",
    "    total_time += end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Avg write time: {}'.format(write_img / len(frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# live demo\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    crop = frame[200:325, 200:325].copy()\n",
    "    cv2.rectangle(frame, (200, 200), (275, 325), (0, 255, 0), 3, 1)\n",
    "    img = cv2.resize(crop, (75, 75))\n",
    "    batch = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "    res = model.predict(batch)\n",
    "    name = labels[np.argmax(res[0])]\n",
    "#     print(name)\n",
    "    \n",
    "    cv2.putText(frame, \"{} | {}\".format(name, res), (40, 40), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import random\n",
    "import time\n",
    "import threading\n",
    "from queue import *\n",
    "class AsynchronousFileReader(threading.Thread):\n",
    "    def __init__(self, fd, my_queue):\n",
    "        assert isinstance(my_queue, Queue)\n",
    "        assert callable(fd.readline)\n",
    "        threading.Thread.__init__(self)\n",
    "        self._fd = fd\n",
    "        self._queue = my_queue\n",
    "    \n",
    "    def run(self):\n",
    "        for line in iter(self._fd.readline, ''):\n",
    "            self._queue.put(line)\n",
    "    \n",
    "    def eof(self):\n",
    "        return not self.is_alive() and self._queue.empty()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
